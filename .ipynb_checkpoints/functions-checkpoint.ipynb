{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook contains the all the functions.\n",
    "\n",
    "#### Normalisation function:\n",
    "This function is used for preprocessing the images for the text extraction.\n",
    "\n",
    "#### OCR function:\n",
    "This function is used for text extraction using AWS Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from PIL import Image\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is for the sample_one. In this the crop size has been changed according to the image. The threshold used is also different, here adaptive threshold is used with gaussian blur, adaptive gaussian threshold and binary threshold.\n",
    "\n",
    "We use the adaptive threshold because, rather than setting a one global threshold value, we let the algorithm calculate the threshold for small regions of the image. Thus, we end up having various threshold values for different regions of the image, which is great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_norm_sample_one(img_path):\n",
    "    # Read image using opencv\n",
    "    img = cv2.imread(img_path)\n",
    "    file_name = os.path.basename(img_path).split('.')[0]\n",
    "    file_name = file_name.split()[0]\n",
    "    current_dir = os.getcwd()\n",
    "    output_dir = os.path.join(current_dir, 'normalised')\n",
    "    \n",
    "    #cropp\n",
    "    img = img[0:270, 0:982]\n",
    "    \n",
    "    #convert into grayscale\n",
    "    #img = cv2.imread(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Rescaling an image , need to scale the image to a larger size to recognize small characters\n",
    "    img = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    #denoising\n",
    "    img = cv2.fastNlMeansDenoising(img, None, 10, 7, 21)\n",
    "    \n",
    "    # Apply dilation and erosion to remove some noise\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img = cv2.erode(img, kernel, iterations=1)\n",
    "    \n",
    "    #apply threshold\n",
    "    img = cv2.adaptiveThreshold(cv2.GaussianBlur(img, (1, 1), 0), 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                cv2.THRESH_BINARY, 31, 2)\n",
    "    \n",
    "    \n",
    "    save_path = os.path.join(output_dir, file_name + \".jpg\")\n",
    "    cv2.imwrite(save_path, img)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for sample two different cropp and thresholds are used depending upon the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_norm_sample_two(img_path):\n",
    "    # Read image using opencv\n",
    "    img = cv2.imread(img_path)\n",
    "    file_name = os.path.basename(img_path).split('.')[0]\n",
    "    file_name = file_name.split()[0]\n",
    "    current_dir = os.getcwd()\n",
    "    output_dir = os.path.join(current_dir, 'normalised')\n",
    "    \n",
    "    #cropp\n",
    "    img = img[0:365, 0:982]\n",
    "    \n",
    "    #convert into grayscale\n",
    "    #img = cv2.imread(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Rescaling an image , need to scale the image to a larger size to recognize small characters\n",
    "    img = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    #denoising\n",
    "    img = cv2.fastNlMeansDenoising(img, None, 10, 7, 21)\n",
    "    \n",
    "    # Apply dilation and erosion to remove some noise\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img = cv2.erode(img, kernel, iterations=1)\n",
    "    \n",
    "    #apply threshold\n",
    "    img = cv2.threshold(cv2.GaussianBlur(img, (3, 3), 0), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    save_path = os.path.join(output_dir, file_name + \".jpg\")\n",
    "    cv2.imwrite(save_path, img)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for sample three different cropp and thresholds are used depending upon the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_norm_sample_three(img_path):\n",
    "    # Read image using opencv\n",
    "    img = cv2.imread(img_path)\n",
    "    file_name = os.path.basename(img_path).split('.')[0]\n",
    "    file_name = file_name.split()[0]\n",
    "    current_dir = os.getcwd()\n",
    "    output_dir = os.path.join(current_dir, 'normalised')\n",
    "    \n",
    "    #cropp\n",
    "    img = img[0:425, 0:982]\n",
    "    \n",
    "    #convert into grayscale\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Rescaling an image , need to scale the image to a larger size to recognize small characters\n",
    "    img = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    #denoising\n",
    "    img = cv2.fastNlMeansDenoising(img, None, 10, 7, 21)\n",
    "    \n",
    "    # Apply dilation and erosion to remove some noise\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img = cv2.erode(img, kernel, iterations=1)\n",
    "    \n",
    "    #apply threshold\n",
    "    img = cv2.threshold(cv2.GaussianBlur(img, (7, 7), 0), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    save_path = os.path.join(output_dir, file_name + \".jpg\")\n",
    "    cv2.imwrite(save_path, img)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly depending upon the multiclass images these fucntions can be changed/increased or can be made one for all the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses AWS textract to extract the text from the image. The response gives the metadata of the image in json format and we use the Text element from that to extract our text. The accuracy for AWS Textract is eay better than pytesseract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr(imgpath):\n",
    "    documentName = imgpath\n",
    "    \n",
    "    file_name = os.path.basename(imgpath).split('.')[0]\n",
    "    file_name = file_name.split()[0] + '.jpg'\n",
    "    \n",
    "    #read documnet content\n",
    "    with open(documentName, 'rb') as document:\n",
    "        imageBytes = bytearray(document.read())\n",
    "    \n",
    "    # Amazon Textract client\n",
    "    textract = boto3.client('textract',region_name='<region-name>')\n",
    "    # Call Amazon Textract\n",
    "    response = textract.detect_document_text(Document={'Bytes': imageBytes})\n",
    "    \n",
    "    l = []\n",
    "    d = {}\n",
    "    #Print detected text\n",
    "    for item in response[\"Blocks\"]:\n",
    "        if item[\"BlockType\"] == \"LINE\":\n",
    "            #print(item[\"Text\"])\n",
    "            l.append(item[\"Text\"])\n",
    "            d= {file_name:l}\n",
    "            #df = pd.DataFrame(list(d.items()), columns=['filename', 'text'])\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytesseract\n",
    "\n",
    "For the pytessract to work we need to do add the path so that it can work. We need to install the .exe file for the same, and we can get that from this url, https://github.com/UB-Mannheim/tesseract/wiki .The path is mentioned in the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'<image-folder>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import pytesseract\n",
    " \n",
    "\n",
    "imPath = r'<image-folder>'\n",
    "config = ('-l eng --oem 1 --psm 3')\n",
    " \n",
    "# Read image from disk\n",
    "im = cv2.imread(imPath)\n",
    " \n",
    "# Run tesseract OCR on image\n",
    "text = pytesseract.image_to_string(im, config=config)\n",
    " \n",
    "# Print recognized text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
